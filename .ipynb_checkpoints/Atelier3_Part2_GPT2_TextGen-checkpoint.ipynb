{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ce5ad6c",
   "metadata": {},
   "source": [
    "\n",
    "# ğŸ¤– Atelier 3 â€“ Part 2: Fine-tuning GPT-2 for Text Generation\n",
    "\n",
    "**UniversitÃ© Abdelmalek Essaadi â€“ Master MBD**  \n",
    "**Lab Task:** Train a Transformer (GPT-2) for custom text generation.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© Step 1: Install & Load GPT-2 Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd9b7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install transformers\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8958165c",
   "metadata": {},
   "source": [
    "## ğŸ“„ Step 2: Prepare Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d8d3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path='custom.txt'):\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        self.samples = [f\"{line.strip()} <|endoftext|>\" for line in lines if line.strip()]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "# Example file creation\n",
    "with open(\"custom.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Ø§Ù„Ù…Ù„Ùƒ ÙŠØªØ±Ø£Ø³ Ø§Ø¬ØªÙ…Ø§Ø¹Ù‹Ø§ ÙˆØ²Ø§Ø±ÙŠÙ‹Ø§ Ù…Ù‡Ù…Ù‹Ø§\n",
    "\")\n",
    "    f.write(\"Ø§Ø¬ØªÙ…Ø§Ø¹ Ø¨ÙŠÙ† ÙˆØ²Ø±Ø§Ø¡ Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ© ÙÙŠ Ø§Ù„Ù…ØºØ±Ø¨\n",
    "\")\n",
    "    f.write(\"Ø§Ù„Ù…Ù…Ù„ÙƒØ© ØªØ·ÙˆØ± Ø´Ø±Ø§ÙƒØ§Øª Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø¬Ø¯ÙŠØ¯Ø©\n",
    "\")\n",
    "\n",
    "dataset = CustomDataset()\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9597903",
   "metadata": {},
   "source": [
    "## ğŸ” Step 3: Fine-tune GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b31bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 5e-5\n",
    "MAX_SEQ_LEN = 128\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=10, num_training_steps=EPOCHS * len(loader))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    for i, line in enumerate(loader):\n",
    "        inputs = tokenizer(line[0], return_tensors=\"pt\", max_length=MAX_SEQ_LEN, truncation=True, padding=\"max_length\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            print(f\"Batch {i}: Loss = {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1b4a81",
   "metadata": {},
   "source": [
    "## âœ¨ Step 4: Generate Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106c2a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "prompt = \"Ø§Ù„Ù…ØºØ±Ø¨\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_ids, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.95, temperature=0.9)\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3974d5",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## âœ… Conclusion\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "- Load and fine-tune GPT-2 using Huggingface Transformers\n",
    "- Use a custom Arabic-like dataset\n",
    "- Generate text based on a prompt\n",
    "\n",
    "You can adapt this workflow to generate political summaries, news intros, or educational content in Arabic.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
