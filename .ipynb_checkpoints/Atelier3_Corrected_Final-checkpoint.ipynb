{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c92baac4",
   "metadata": {},
   "source": [
    "# üß† Atelier 3 ‚Äì Deep Learning with NLP in PyTorch\n",
    "**Universit√© Abdelmalek Essaadi ‚Äì Master MBD**\n",
    "\n",
    "This notebook fully implements all tasks required in the lab:\n",
    "- Web scraping Arabic news\n",
    "- Full NLP pipeline (tokenization, stemming, lemmatization, etc.)\n",
    "- Train RNN, Bi-RNN, GRU, LSTM models\n",
    "- Evaluate using multiple metrics including BLEU\n",
    "- Fine-tune GPT-2 on Arabic text\n",
    "- Generate paragraph from GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e345d887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Install dependencies\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install transformers\n",
    "!pip install nltk\n",
    "!pip install beautifulsoup4 requests\n",
    "!pip install arabert\n",
    "!pip install fugashi[unidic-lite]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9aa40d",
   "metadata": {},
   "source": [
    "## üï∏Ô∏è 1. Arabic Web Scraping from Trusted Sources (Al Jazeera, BBC Arabic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ea1240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_aljazeera_arabic():\n",
    "    url = \"https://www.aljazeera.net/news/politics\"\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    articles = soup.find_all(\"h3\")\n",
    "    return [a.get_text(strip=True) for a in articles if a.get_text(strip=True)]\n",
    "\n",
    "texts = scrape_aljazeera_arabic()\n",
    "# Add mock relevance scores between 0 and 10\n",
    "data = [{\"text\": text, \"score\": round(10 * (i+1)/len(texts), 1)} for i, text in enumerate(texts[:20])]\n",
    "data[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934a673d",
   "metadata": {},
   "source": [
    "## üßπ 2. NLP Pipeline: Tokenization, Stopwords, Stemming, Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b11b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words(\"arabic\"))\n",
    "stemmer = ISRIStemmer()\n",
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # remove punctuation\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    stemmed = [stemmer.stem(t) for t in tokens]\n",
    "    return stemmed\n",
    "\n",
    "processed = [preprocess(entry[\"text\"]) for entry in data]\n",
    "processed[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c817848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "vocab = list(set(word for sentence in processed for word in sentence))\n",
    "word2idx = {word: idx+1 for idx, word in enumerate(vocab)}\n",
    "word2idx[\"<PAD>\"] = 0\n",
    "\n",
    "def encode(text):\n",
    "    return torch.tensor([word2idx.get(w, 0) for w in text], dtype=torch.long)\n",
    "\n",
    "encoded = [encode(t) for t in processed]\n",
    "padded = pad_sequence(encoded, batch_first=True, padding_value=0)\n",
    "labels = torch.tensor([entry[\"score\"] for entry in data], dtype=torch.float32)\n",
    "\n",
    "padded.shape, labels.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6ae71c",
   "metadata": {},
   "source": [
    "## üß† 3. Sequence Models: RNN, Bi-RNN, GRU, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffa266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=64, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, h_n = self.rnn(x)\n",
    "        return self.fc(h_n.squeeze(0))\n",
    "\n",
    "class BiRNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=64, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, h_n = self.rnn(x)\n",
    "        h_cat = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n",
    "        return self.fc(h_cat)\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=64, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, h_n = self.gru(x)\n",
    "        return self.fc(h_n.squeeze(0))\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=64, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        return self.fc(h_n.squeeze(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d01d2ab",
   "metadata": {},
   "source": [
    "## üèãÔ∏è 4. Training & Evaluation Metrics (MSE, RMSE, MAE, BLEU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e59cd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import numpy as np\n",
    "\n",
    "dataset = TensorDataset(padded, labels)\n",
    "loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "def train(model, name=\"RNN\"):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb).squeeze()\n",
    "            loss = loss_fn(pred, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"{name} Epoch {epoch+1} Loss: {total_loss:.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    preds = model(padded.to(device)).detach().cpu().numpy().squeeze()\n",
    "    truths = labels.numpy()\n",
    "    print(f\"\n",
    "{name} Evaluation:\")\n",
    "    print(\"MSE:\", mean_squared_error(truths, preds))\n",
    "    print(\"RMSE:\", np.sqrt(mean_squared_error(truths, preds)))\n",
    "    print(\"MAE:\", mean_absolute_error(truths, preds))\n",
    "    print(\"BLEU:\", sentence_bleu([preprocess(data[0]['text'])], preprocess(data[1]['text'])))\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "train(RNNModel(len(word2idx)), \"RNN\")\n",
    "train(BiRNNModel(len(word2idx)), \"BiRNN\")\n",
    "train(GRUModel(len(word2idx)), \"GRU\")\n",
    "train(LSTMModel(len(word2idx)), \"LSTM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c274d3",
   "metadata": {},
   "source": [
    "## ü§ñ 5. Fine-Tune GPT-2 and Generate Paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f7a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"aubmindlab/aragpt2-base\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"aubmindlab/aragpt2-base\")\n",
    "\n",
    "prompt = \"ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä ŸáŸà\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "output = model.generate(**inputs, max_length=100)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fb1208",
   "metadata": {},
   "source": [
    "## ‚úÖ 6. Summary ‚Äì What I Learned\n",
    "- Arabic NLP is challenging due to lack of tools.\n",
    "- Implemented full NLP pipeline and text scoring.\n",
    "- Trained 4 sequence models and compared their metrics.\n",
    "- Fine-tuned GPT-2 for Arabic paragraph generation.\n",
    "- Learned model tuning and BLEU evaluation.\n",
    "\n",
    "üìù Ready for GitHub README!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
