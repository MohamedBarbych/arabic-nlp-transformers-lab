
# ğŸ§  Arabic NLP â€“ Deep Learning Lab (Atelier 3)

## ğŸ“ UniversitÃ© Abdelmalek Essaadi â€“ Master MBD  
**Instructor**: Pr. Elaachak Lotfi  
**Author**: Mohamed BARBYCH

---

## ğŸ“š Description

This project is a complete deep learning lab for **Arabic Natural Language Processing (NLP)** using both:
- **Sequence Models** (RNN, Bi-RNN, GRU, LSTM)
- **Transformer-based GPT-2 fine-tuning** for text generation.

It is divided into two main parts:
- **Part 1**: Classification of Arabic news headlines with semantic scoring.
- **Part 2**: Fine-tuning an Arabic GPT-2 model (`aragpt2-base`) to generate realistic Arabic text.

---

## ğŸ“ Folder Structure

```
.
â”œâ”€â”€ Atelier3_Part1_1.ipynb            # Scraping, scoring, preprocessing
â”œâ”€â”€ Atelier3_Part1_2.ipynb            # RNN, GRU, LSTM classification + metrics
â”œâ”€â”€ Atelier3_Part2_ArabicGPT2.ipynb   # GPT-2 fine-tuning and generation
â”œâ”€â”€ arabic_data.txt                   # Custom Arabic sentences for generation
â”œâ”€â”€ custom.txt                        # Short demo sentences for quick tuning
â”œâ”€â”€ data_scraped.csv                 # Scraped headlines from Hespress
â”œâ”€â”€ data_semantically_scored.csv     # Labeled headlines with relevance scores
```

---

## ğŸ§ª Part 1 â€“ Classification with Sequence Models

### âœ… Objective
Build a semantic classifier that evaluates the **relevance of Arabic news** headlines.

### ğŸ”§ Models Used:
- RNN
- Bidirectional RNN
- GRU
- LSTM

### ğŸ“Š Preprocessing Techniques:
- Arabic tokenization using `ToktokTokenizer`
- Stemming using `ISRIStemmer`
- Stopword removal with NLTK
- Discretization of semantic scores into 5 bins

### ğŸ§  Results (Summary):

| Model   | Accuracy | Observations |
|---------|----------|--------------|
| RNN     | ~45%     | Bias toward dominant class (1) |
| Bi-RNN  | ~48%     | Slight improvement due to bidirectionality |
| GRU     | ~43%     | Requires longer training & more data |
| LSTM    | ~43%     | Struggled with short TF-IDF input vectors |

â¡ï¸ **Conclusion**: While the models are functional, performance is limited by:
- Small dataset size
- Label imbalance
- TF-IDF input limitations for sequential architectures

---

## ğŸ¤– Part 2 â€“ Arabic GPT-2 Fine-Tuning

### âœ… Objective
Use the pre-trained **Arabic GPT-2 (aragpt2-base)** model to generate new political/news content.

### ğŸ“ Dataset
Used `arabic_data.txt` with short political-style statements:
```
Ø§Ù„Ù…Ù„Ùƒ ÙŠØªØ±Ø£Ø³ Ø§Ø¬ØªÙ…Ø§Ø¹Ù‹Ø§ ÙˆØ²Ø§Ø±ÙŠÙ‹Ø§ Ù…Ù‡Ù…Ù‹Ø§
Ø§Ù„Ù…Ù…Ù„ÙƒØ© ØªØ·ÙˆØ± Ø´Ø±Ø§ÙƒØ§Øª Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø¬Ø¯ÙŠØ¯Ø©
...
```

### ğŸ” Training Loop
- Model: `aubmindlab/aragpt2-base`
- Epochs: 3
- Loss: Decreased steadily
- Tokenizer padding set to EOS to prevent errors

### âœ¨ Example Generation

**Input**: `Ø§Ù„Ù…ØºØ±Ø¨`  
**Output**: *(after fine-tuning)*  
```
Ø§Ù„Ù…ØºØ±Ø¨ ÙŠÙˆØ§ØµÙ„ ØªÙˆØ³ÙŠØ¹ Ø´Ø±Ø§ÙƒØ§ØªÙ‡ Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠØ© ÙÙŠ Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø¥ÙØ±ÙŠÙ‚ÙŠØ© Ù…Ø¹ ØªØ¹Ø²ÙŠØ² Ø¯ÙˆØ±Ù‡ Ø§Ù„Ø¯Ø¨Ù„ÙˆÙ…Ø§Ø³ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¯ÙˆÙ„ÙŠ.
```

â¡ï¸ GPT-2 trained on Arabic text produced **semantically and syntactically correct** completions.

---

## âœ… What I Learned

- How to collect and clean Arabic text data
- How to build RNN-based classifiers using PyTorch
- How tokenization, stopwords, and class imbalance affect performance
- How to fine-tune pretrained transformer models for Arabic text generation
- How to control text creativity using sampling methods (`top_k`, `top_p`, `temperature`)

---

## ğŸš€ How to Run

### ğŸ”— On Google Colab:
```python
!pip install transformers
```
Then open `.ipynb` files and run the cells as instructed.

---

## ğŸ“¤ Deployment

You can later:
- Host this as a demo app using Gradio or Streamlit
- Connect it to an Arabic headline auto-generator

---

## ğŸ“ References

- [Huggingface Transformers](https://huggingface.co)
- [AUB Mind Lab â€“ aragpt2-base](https://huggingface.co/aubmindlab/aragpt2-base)
- [Gist inspiration for GPT-2 fine-tuning](https://gist.github.com/mf1024/3df214d2f17f3dcc56450ddf0d5a4cd7)

---

## âœ… Final Note

This lab was a great opportunity to dive deep into **Arabic NLP**, mixing classic sequence modeling with modern transformers.

**Thank you!**
