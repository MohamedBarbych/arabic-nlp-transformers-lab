{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dfb2684",
   "metadata": {},
   "source": [
    "\n",
    "# ğŸ§  Deep Learning â€“ Atelier 3 â€“ Arabic News Classification\n",
    "\n",
    "**UniversitÃ© Abdelmalek Essaadi â€“ LSI**  \n",
    "**Instructor:** Pr. ELAACHAK LOTFI  \n",
    "\n",
    "This notebook covers:\n",
    "1. Scraping Arabic political news headlines using Selenium + BeautifulSoup.\n",
    "2. Assigning a relevance score (0 to 10) to each title using rule-based heuristics.\n",
    "3. Saving the dataset to a CSV file for further NLP model training.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“° Step 1: Web Scraping with Selenium & BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ca22218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /home/med/jupyter_env/lib/python3.12/site-packages (4.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/med/jupyter_env/lib/python3.12/site-packages (4.12.3)\n",
      "Requirement already satisfied: pandas in /home/med/jupyter_env/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: nltk in /home/med/jupyter_env/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: webdriver-manager in /home/med/jupyter_env/lib/python3.12/site-packages (4.0.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /home/med/jupyter_env/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
      "Requirement already satisfied: trio~=0.17 in /home/med/jupyter_env/lib/python3.12/site-packages (from selenium) (0.29.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /home/med/jupyter_env/lib/python3.12/site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /home/med/jupyter_env/lib/python3.12/site-packages (from selenium) (2024.8.30)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /home/med/jupyter_env/lib/python3.12/site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /home/med/jupyter_env/lib/python3.12/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/med/jupyter_env/lib/python3.12/site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/med/jupyter_env/lib/python3.12/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/med/jupyter_env/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/med/jupyter_env/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/med/jupyter_env/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: click in /home/med/jupyter_env/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/med/jupyter_env/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/med/jupyter_env/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/med/jupyter_env/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: requests in /home/med/jupyter_env/lib/python3.12/site-packages (from webdriver-manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in /home/med/jupyter_env/lib/python3.12/site-packages (from webdriver-manager) (1.1.0)\n",
      "Requirement already satisfied: packaging in /home/med/jupyter_env/lib/python3.12/site-packages (from webdriver-manager) (24.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/med/jupyter_env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /home/med/jupyter_env/lib/python3.12/site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers in /home/med/jupyter_env/lib/python3.12/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /home/med/jupyter_env/lib/python3.12/site-packages (from trio~=0.17->selenium) (3.10)\n",
      "Requirement already satisfied: outcome in /home/med/jupyter_env/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /home/med/jupyter_env/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in /home/med/jupyter_env/lib/python3.12/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /home/med/jupyter_env/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/med/jupyter_env/lib/python3.12/site-packages (from requests->webdriver-manager) (3.4.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /home/med/jupyter_env/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ø¥Ø³ÙˆØ§ØªÙŠÙ†ÙŠ ØªØ±ÙØ¶ Ù…Ù†Ø§ÙˆØ±Ø§Øª Ø¬Ù†ÙˆØ¨ Ø¥ÙØ±ÙŠÙ‚ÙŠØ§</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ù…Ù† Ø§Ù„ØªØ¯Ø¨ÙŠØ± Ø¥Ù„Ù‰ Ø§Ù„ØªØºÙŠÙŠØ±.. Ø§Ù„Ù…ØºØ±Ø¨ ÙŠÙƒØ«Ù Ø§Ù„ØªØ­Ø±ÙƒØ§Øª ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ù‡Ù„ ØªÙ†Ø¬Ø­ Ø§Ù„ÙˆØ³Ø§Ø·Ø© Ø§Ù„Ø£Ù…Ø±ÙŠÙƒÙŠØ© Ø¨ÙŠÙ† Ø§Ù„Ù…ØºØ±Ø¨ ÙˆØ§Ù„Ø¬Ø²Ø§Ø¦Ø± ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ù…Ù„Ù Ø§Ù„ØµØ­Ø±Ø§Ø¡: Ù…Ù† Ù†Ø¶Ø¬ Ø§Ù„Ù…Ø¨Ø§Ø¯Ø±Ø© Ø§Ù„Ù…ØºØ±Ø¨ÙŠØ© Ø¥Ù„Ù‰ Ø§Ø®ØªØ¨...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ØªÙ‡Ø¯ÙŠØ¯Ø§Øª ÙˆØ²ÙŠØ± Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© Ø§Ù„ÙØ±Ù†Ø³ÙŠ ØªØ¹Ù…Ù‚ Ø¹Ø²Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  score\n",
       "0                 Ø¥Ø³ÙˆØ§ØªÙŠÙ†ÙŠ ØªØ±ÙØ¶ Ù…Ù†Ø§ÙˆØ±Ø§Øª Ø¬Ù†ÙˆØ¨ Ø¥ÙØ±ÙŠÙ‚ÙŠØ§      0\n",
       "1  Ù…Ù† Ø§Ù„ØªØ¯Ø¨ÙŠØ± Ø¥Ù„Ù‰ Ø§Ù„ØªØºÙŠÙŠØ±.. Ø§Ù„Ù…ØºØ±Ø¨ ÙŠÙƒØ«Ù Ø§Ù„ØªØ­Ø±ÙƒØ§Øª ...      0\n",
       "2  Ù‡Ù„ ØªÙ†Ø¬Ø­ Ø§Ù„ÙˆØ³Ø§Ø·Ø© Ø§Ù„Ø£Ù…Ø±ÙŠÙƒÙŠØ© Ø¨ÙŠÙ† Ø§Ù„Ù…ØºØ±Ø¨ ÙˆØ§Ù„Ø¬Ø²Ø§Ø¦Ø± ...      0\n",
       "3  Ù…Ù„Ù Ø§Ù„ØµØ­Ø±Ø§Ø¡: Ù…Ù† Ù†Ø¶Ø¬ Ø§Ù„Ù…Ø¨Ø§Ø¯Ø±Ø© Ø§Ù„Ù…ØºØ±Ø¨ÙŠØ© Ø¥Ù„Ù‰ Ø§Ø®ØªØ¨...      0\n",
       "4  ØªÙ‡Ø¯ÙŠØ¯Ø§Øª ÙˆØ²ÙŠØ± Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© Ø§Ù„ÙØ±Ù†Ø³ÙŠ ØªØ¹Ù…Ù‚ Ø¹Ø²Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "!pip install selenium beautifulsoup4 pandas nltk webdriver-manager\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.hespress.com/politique\"\n",
    "\n",
    "# Set up headless Chrome\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# Scroll and load content\n",
    "driver.get(url)\n",
    "end_time = time.time() + 60  # Scroll for 60 seconds\n",
    "\n",
    "while time.time() < end_time:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "\n",
    "# Parse page source\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "driver.quit()\n",
    "\n",
    "# Extract article titles\n",
    "titles = soup.find_all('a', class_='stretched-link')\n",
    "title_texts = [title.get('title') for title in titles if title.get('title')]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(title_texts, columns=['title'])\n",
    "df['score'] = 0\n",
    "df.to_csv(\"data_scraped.csv\", index=False)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cfaff5",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ğŸ§® Step 2: Assigning Relevance Scores\n",
    "\n",
    "Using basic keyword matching to classify news articles as internal (score closer to 0) or external (score closer to 10).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "422c03ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/med/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ø¥Ø³ÙˆØ§ØªÙŠÙ†ÙŠ ØªØ±ÙØ¶ Ù…Ù†Ø§ÙˆØ±Ø§Øª Ø¬Ù†ÙˆØ¨ Ø¥ÙØ±ÙŠÙ‚ÙŠØ§</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ù…Ù† Ø§Ù„ØªØ¯Ø¨ÙŠØ± Ø¥Ù„Ù‰ Ø§Ù„ØªØºÙŠÙŠØ±.. Ø§Ù„Ù…ØºØ±Ø¨ ÙŠÙƒØ«Ù Ø§Ù„ØªØ­Ø±ÙƒØ§Øª ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ù‡Ù„ ØªÙ†Ø¬Ø­ Ø§Ù„ÙˆØ³Ø§Ø·Ø© Ø§Ù„Ø£Ù…Ø±ÙŠÙƒÙŠØ© Ø¨ÙŠÙ† Ø§Ù„Ù…ØºØ±Ø¨ ÙˆØ§Ù„Ø¬Ø²Ø§Ø¦Ø± ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ù…Ù„Ù Ø§Ù„ØµØ­Ø±Ø§Ø¡: Ù…Ù† Ù†Ø¶Ø¬ Ø§Ù„Ù…Ø¨Ø§Ø¯Ø±Ø© Ø§Ù„Ù…ØºØ±Ø¨ÙŠØ© Ø¥Ù„Ù‰ Ø§Ø®ØªØ¨...</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ØªÙ‡Ø¯ÙŠØ¯Ø§Øª ÙˆØ²ÙŠØ± Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© Ø§Ù„ÙØ±Ù†Ø³ÙŠ ØªØ¹Ù…Ù‚ Ø¹Ø²Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…...</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  score\n",
       "0                 Ø¥Ø³ÙˆØ§ØªÙŠÙ†ÙŠ ØªØ±ÙØ¶ Ù…Ù†Ø§ÙˆØ±Ø§Øª Ø¬Ù†ÙˆØ¨ Ø¥ÙØ±ÙŠÙ‚ÙŠØ§    2.5\n",
       "1  Ù…Ù† Ø§Ù„ØªØ¯Ø¨ÙŠØ± Ø¥Ù„Ù‰ Ø§Ù„ØªØºÙŠÙŠØ±.. Ø§Ù„Ù…ØºØ±Ø¨ ÙŠÙƒØ«Ù Ø§Ù„ØªØ­Ø±ÙƒØ§Øª ...    3.0\n",
       "2  Ù‡Ù„ ØªÙ†Ø¬Ø­ Ø§Ù„ÙˆØ³Ø§Ø·Ø© Ø§Ù„Ø£Ù…Ø±ÙŠÙƒÙŠØ© Ø¨ÙŠÙ† Ø§Ù„Ù…ØºØ±Ø¨ ÙˆØ§Ù„Ø¬Ø²Ø§Ø¦Ø± ...    3.0\n",
       "3  Ù…Ù„Ù Ø§Ù„ØµØ­Ø±Ø§Ø¡: Ù…Ù† Ù†Ø¶Ø¬ Ø§Ù„Ù…Ø¨Ø§Ø¯Ø±Ø© Ø§Ù„Ù…ØºØ±Ø¨ÙŠØ© Ø¥Ù„Ù‰ Ø§Ø®ØªØ¨...    7.5\n",
       "4  ØªÙ‡Ø¯ÙŠØ¯Ø§Øª ÙˆØ²ÙŠØ± Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© Ø§Ù„ÙØ±Ù†Ø³ÙŠ ØªØ¹Ù…Ù‚ Ø¹Ø²Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…...    3.5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "stop_words = set(nltk.corpus.stopwords.words('arabic'))\n",
    "\n",
    "external_keywords = ['Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„', 'Ø§Ù„Ø£Ù…Ù…', 'Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©', 'Ø§Ù„Ø£Ù…Ù… Ø§Ù„Ù…ØªØ­Ø¯Ø©', 'ÙØ±Ù†Ø³Ø§', 'Ø£Ù…Ø±ÙŠÙƒØ§', 'Ø§Ù„Ø¨ÙŠØª Ø§Ù„Ø£Ø¨ÙŠØ¶', 'Ø§Ù„Ø£ÙˆØ±ÙˆØ¨ÙŠ', 'Ø§Ù„Ø¯ÙˆÙ„ÙŠ']\n",
    "internal_keywords = ['Ø§Ù„Ù…ØºØ±Ø¨', 'Ø§Ù„Ø±Ø¨Ø§Ø·', 'Ø§Ù„Ø­ÙƒÙˆÙ…Ø©', 'Ù…Ø¬Ù„Ø³', 'ÙˆØ²ÙŠØ±', 'Ø§Ù„Ù…ØºØ±Ø¨ÙŠØ©', 'Ø¬Ù‡Ø©', 'Ø§Ù„Ù…Ù„Ùƒ', 'Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©']\n",
    "\n",
    "df = pd.read_csv('data_scraped.csv')\n",
    "\n",
    "def compute_score(title):\n",
    "    words = tokenizer.tokenize(str(title))\n",
    "    keywords = [word for word in words if word not in stop_words and word.isalpha()]\n",
    "    \n",
    "    # Initial score\n",
    "    score = 0.0\n",
    "\n",
    "    # Keyword frequency\n",
    "    ext_hits = sum(1 for w in external_keywords if w in title)\n",
    "    int_hits = sum(1 for w in internal_keywords if w in title)\n",
    "    \n",
    "    score += ext_hits * 2\n",
    "    score -= int_hits * 1\n",
    "\n",
    "    # Add richness score\n",
    "    score += 0.5 * len(keywords)\n",
    "\n",
    "    # Bonus for long titles\n",
    "    if len(keywords) > 8:\n",
    "        score += 1\n",
    "\n",
    "    # Normalize to [0, 10]\n",
    "    score = min(10, max(0, round(score, 1)))\n",
    "    \n",
    "    return ' '.join(keywords), score\n",
    "\n",
    "df[['keywords', 'score']] = df['title'].apply(lambda x: pd.Series(compute_score(x)))\n",
    "df.to_csv(\"data_semantically_scored.csv\", index=False)\n",
    "df[['title', 'score']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0092adb2",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary\n",
    "\n",
    "- We collected Arabic news titles from Hespress (Politics section).\n",
    "- We applied basic NLP techniques to assign a relevance score between 0â€“10.\n",
    "- Saved the processed data to `titles-scored.csv`.\n",
    "\n",
    "You can now proceed to:\n",
    "- Preprocessing pipeline (tokenization, lemmatization, etc.)\n",
    "- Model training: RNN, Bi-RNN, GRU, LSTM\n",
    "- Part 2: Fine-tuning GPT-2 for Arabic text generation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
